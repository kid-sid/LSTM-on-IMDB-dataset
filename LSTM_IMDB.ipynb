{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_IMDB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr4cugH3mZ_X"
      },
      "source": [
        "# Credits: https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
        "# LSTM for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjf2yM5GF4_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac272af9-e5b8-41d1-ea17-99abdd247600"
      },
      "source": [
        "#Refer: https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification\n",
        "\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The `nb_words` argument in `load_data` has been renamed `num_words`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYEE6ts7GAjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc9966a-33fa-4273-c82a-24f742264dd8"
      },
      "source": [
        "print(X_train[1])\n",
        "# print(X_train[20])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxQOEfQEr2D6"
      },
      "source": [
        "#### How the words are encoded?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0htAlgerdHl"
      },
      "source": [
        "    Let there is a data corpus of reviews\n",
        "    x1: review1, x2: review2, x3:review3, ......\n",
        "    review1 = ([w1,w2,w3,w4,w5,w6] , y1), review2 = ([w1,w2,w3,w4,w5,w6,w7,w8,w9,w10], y2), .........\n",
        "    \n",
        "    1.Build a set of all the words in the complete reviews corpus (let v)\n",
        "    2.compute the frequency of all the words (word : frequency)\n",
        "    3.Sort by frequency in decending order\n",
        "    4.Give all the index numbers (Whenever we see a word, we will replace it with its index)\n",
        "    5.let is:50k , a:49k , for:20k , this:5k , food:50 , dogs:10 = [1,2,3,4,5]\n",
        "      e.g: This food is for dogs : We can represent the sentence as [4,5,1,3,6] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKNpqUPmUnYn",
        "outputId": "d3011bd1-8698-46cc-c9d4-1b686a89bb1c"
      },
      "source": [
        "print(type(X_train[1]))       \n",
        "print(len(X_train[1]))   # Review1 contains 189 words and it was encoded in the above cell\n",
        "print(len(X_train[2]))   # Review2 contains 141 words and it was encoded in the above cell"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "189\n",
            "141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57N6TyKLH-Pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a90415de-a7b5-402c-b349-35eaa3567a9d"
      },
      "source": [
        "# truncate and/or pad input sequences\n",
        "max_review_length = 600\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)  # We are performing a pre padding with max_length=600 \n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "print(X_train.shape) # (25000 reviews, 600words from each review)\n",
        "print(X_train[1])    # review1 is having 189 words, but we are adding 0 at other 411 places and it is a pre padding operation."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 600)\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    1  194 1153  194    2   78  228    5    6\n",
            " 1463 4369    2  134   26    4  715    8  118 1634   14  394   20   13\n",
            "  119  954  189  102    5  207  110 3103   21   14   69  188    8   30\n",
            "   23    7    4  249  126   93    4  114    9 2300 1523    5  647    4\n",
            "  116    9   35    2    4  229    9  340 1322    4  118    9    4  130\n",
            " 4901   19    4 1002    5   89   29  952   46   37    4  455    9   45\n",
            "   43   38 1543 1905  398    4 1649   26    2    5  163   11 3215    2\n",
            "    4 1153    9  194  775    7    2    2  349 2637  148  605    2    2\n",
            "   15  123  125   68    2    2   15  349  165 4362   98    5    4  228\n",
            "    9   43    2 1157   15  299  120    5  120  174   11  220  175  136\n",
            "   50    9 4373  228    2    5    2  656  245 2350    5    4    2  131\n",
            "  152  491   18    2   32    2 1212   14    9    6  371   78   22  625\n",
            "   64 1382    9    8  168  145   23    4 1690   15   16    4 1355    5\n",
            "   28    6   52  154  462   33   89   78  285   16  145   95]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrjG0H54jUBY"
      },
      "source": [
        "### Why should we pad?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqxJkNwZjaEJ"
      },
      "source": [
        "    Every review is of different length.\n",
        "    For e.g:  review1: The movie was nice. \n",
        "              review2: I liked the movie, it was amazing, \n",
        "              review3: It was the worst movie I have seen in my life\n",
        "    \n",
        "    We have selected that our review length should be of 600 words.\n",
        "    So we need to add 0 except at the words present in the review to make all the reviews of same length.\n",
        "    If we don't pad, then we need to send all the words of a review at a time and then another review and so on. This will make the process too much slow.\n",
        "    So we can do batch update to speed up the process by sending more words at a time with a batch size.\n",
        "\n",
        "    e.g: review1 = [0,0,0,0,0,w1, w2, w3, w4, w5,.........,w40]   Total 42 words are there and after padding the length became 45\n",
        "         review2 = [0,0,0,w1, w2, w3, w4, w5,.........,w42]   Total 42 words are there and after padding the length became 45\n",
        "         \n",
        "         Instead of sending all the words in a review one after another(which will make the process extremely slow), \n",
        "         we can specify a batch and send the words in a batch, like {x11,x21,x31,x41,x51} , {x12,x22,x32,x42,x52} , .... so on. Here batch size = 5\n",
        "         Sending the words in a batch will speed up the process.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CquzlqrOIYGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ce2e35-8bed-4507-8431-5f5dcafc9ac3"
      },
      "source": [
        "# create the model\n",
        "embedding_vector_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words+1, embedding_vector_length, input_length=max_review_length))  # Embedding layer converts the integers into vectors.\n",
        "''' In above line we are just embedding our top 5000 words (which is our input) with an output vector length of 32 for 600 reviews\n",
        "    Number of embeddings = (number of words * output vector size) = 5000*32 = 160000 '''\n",
        "model.add(LSTM(100)) \n",
        "'''100 LSTMs are there, all the 32 dimension vector go to each LSTM and each LSTM gives a different output.\n",
        "   m = 32 = (inputs) , n = 100 = (outputs),    Number of parameters = 4(nm + sqr(n) + n) = 53200'''\n",
        "#Refer: https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))    # \n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Embedding layer converts the integers into vectors. e.g: [[4],[20]] = []\n",
        "# The number of parameters of LSTM, taking input vectors of size m and giving output vectors of size n with bias factor is 4(nm+n2+n)\n",
        "# Our dense sigmoid layer is connected to 100 weights and we add 1 bias to it, so it becomes 101"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 600, 32)           160032    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,333\n",
            "Trainable params: 213,333\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_FT0dPNIeLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3654cfbd-814e-4af2-a616-c8ff8e29c5df"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=10, batch_size=64) \n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 277s 662ms/step - loss: 0.6138 - accuracy: 0.6345\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 261s 668ms/step - loss: 0.3168 - accuracy: 0.8733\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 259s 663ms/step - loss: 0.2791 - accuracy: 0.8861\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 259s 661ms/step - loss: 0.2557 - accuracy: 0.9013\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 259s 662ms/step - loss: 0.1895 - accuracy: 0.9323\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 260s 664ms/step - loss: 0.1742 - accuracy: 0.9348\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 259s 663ms/step - loss: 0.1519 - accuracy: 0.9460\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 259s 663ms/step - loss: 0.1252 - accuracy: 0.9570\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 259s 662ms/step - loss: 0.1174 - accuracy: 0.9601\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 258s 660ms/step - loss: 0.1095 - accuracy: 0.9607\n",
            "Accuracy: 86.52%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgVm16Yt-pef"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}